{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"hubName = \"NekoFi/llama-3-indotuned-v0\"\nrepo_name = \"ukung/Llama3-IndoTunned-GGUF\"\nfile_name = \"Llama3-IndoTunned\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade huggingface_hub\n# !git clone https://github.com/ggerganov/llama.cpp.git\n# !pip install -r llama.cpp/requirements.txt\n# %cd llama.cpp\n# !make\n# %cd ..\n\n# !pip install --upgrade numpy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nfrom pathlib import Path\nfrom huggingface_hub import HfApi, Repository, login\nfrom huggingface_hub.hf_api import HfFolder\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_latest_model_path():\n    hub_dir = Path(\"/root/.cache/huggingface/hub\")\n    \n    # Find all model directories in the hub directory\n    model_dirs = [d for d in hub_dir.iterdir() if d.is_dir()]\n    \n    # Get the latest created model directory\n    latest_model_dir = max(model_dirs, key=os.path.getctime)\n    \n    # Find the snapshots directory within the latest model directory\n    snapshots_dir = latest_model_dir / \"snapshots\"\n    \n    # Get a random ID directory within the snapshots directory\n    snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]\n    if not snapshot_dirs:\n        raise FileNotFoundError(\"No snapshot directories found.\")\n    \n    random_snapshot_dir = snapshot_dirs[0]  # Assuming you want just the first one found\n    \n    # Return the full path\n    return str(random_snapshot_dir)\n\ndef get_latest_model_base_path():\n    hub_dir = Path(\"/root/.cache/huggingface/hub\")\n    \n    # Find all model directories in the hub directory\n    model_dirs = [d for d in hub_dir.iterdir() if d.is_dir()]\n    \n    # Get the latest created model directory\n    latest_model_dir = max(model_dirs, key=os.path.getctime)\n    \n    # Return the full path to the latest model directory\n    return str(latest_model_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HfFolder.save_token('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')\ntoken = \"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\"  # Ganti dengan token API kamu\nlogin(token)\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch.cuda.empty_cache()\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(hubName,trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(hubName,trust_remote_code=True)\ntokenizer = None\nmodel = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"api = HfApi()\napi.create_repo(repo_name, exist_ok=True)\ncacheFolder = get_latest_model_path()\nbase_path = get_latest_model_base_path()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python llama.cpp/convert-hf-to-gguf.py {cacheFolder} --outfile /root/.cache/f16.bin --outtype f16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_methods = [\n    \"q2_k\", \n    \"q3_k_l\", \n    \"q3_k_m\", \"q3_k_s\",\n    \"q4_0\", \"q4_1\", \"q4_k_m\", \"q4_k_s\",\n    \"q5_0\", \"q5_1\", \"q5_k_m\", \"q5_k_s\",\n    \"q6_k\", \"q8_0\"\n]\nfile_path_f16 = \"/root/.cache/f16.bin\"\nfor method in quant_methods:\n    fileName = f\"{file_name}-{method}.gguf\"\n    !./llama.cpp/quantize {file_path_f16} {fileName} {method}\n    api.upload_file(path_or_fileobj=fileName, path_in_repo=fileName, repo_id=repo_name)\n    !rm {fileName}\n    !ls -lah\n    \napi.upload_file(path_or_fileobj=file_path_f16, path_in_repo=f\"{file_name}-f16.bin\", repo_id=repo_name)","metadata":{"execution":{"iopub.status.idle":"2024-05-20T06:59:49.523548Z","shell.execute_reply.started":"2024-05-20T04:54:33.625801Z","shell.execute_reply":"2024-05-20T06:59:49.521494Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ukung/Llama3-IndoTunned-GGUF/commit/97075fe2d79673192612920c1e9aadbd4e9aba23', commit_message='Upload Llama3-IndoTunned-f16.bin with huggingface_hub', commit_description='', oid='97075fe2d79673192612920c1e9aadbd4e9aba23', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"!rm -rf {base_path}\n!rm /root/.cache/f16.bin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING","metadata":{}},{"cell_type":"code","source":"!pip install ctransformers","metadata":{"execution":{"iopub.status.busy":"2024-05-20T07:19:44.146476Z","iopub.execute_input":"2024-05-20T07:19:44.146937Z","iopub.status.idle":"2024-05-20T07:19:59.256250Z","shell.execute_reply.started":"2024-05-20T07:19:44.146897Z","shell.execute_reply":"2024-05-20T07:19:59.254883Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Collecting ctransformers\n  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from ctransformers) (0.23.0)\nRequirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from ctransformers) (9.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->ctransformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (2024.2.2)\nDownloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ctransformers\nSuccessfully installed ctransformers-0.2.27\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}